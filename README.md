# TCC
## üß© Arquitetura, L√≥gica e Etapas do Pipeline

O pipeline foi modularizado em blocos independentes para garantir a escalabilidade e facilitar a manuten√ß√£o do c√≥digo. Abaixo est√° o detalhamento t√©cnico de cada etapa acionada pelo orquestrador:

### üì• Etapa 1: Aquisi√ß√£o
* **Objetivo:** Coleta prim√°ria dos dados a partir de identificadores.
* **L√≥gica de Programa√ß√£o e Pontos-Chave:**
  * **Manipula√ß√£o de Dados:** Utiliza a biblioteca `pandas` para ler a planilha Excel inicial e iterar sobre a coluna de DOIs.
  * **Requisi√ß√µes Web:** Emprega bibliotecas como `requests` ou `urllib` para acessar os reposit√≥rios dos artigos.
  * **Tratamento de Exce√ß√µes (Error Handling):** Ponto cr√≠tico do script. Implementa blocos `try-except` para lidar com links quebrados (Erro 404), bloqueios de acesso (Erro 403) ou tempos de limite de conex√£o (Timeouts), garantindo que o orquestrador n√£o pare de funcionar se um download falhar.
  * **Controle de Taxa (Rate Limiting):** Uso de pausas (`time.sleep`) entre as requisi√ß√µes para evitar o bloqueio do IP pelos servidores acad√™micos.

### üßπ Etapa 2: Normaliza√ß√£o
* **Objetivo:** Limpeza e padroniza√ß√£o do texto bruto.
* **L√≥gica de Programa√ß√£o e Pontos-Chave:**
  * **Extra√ß√£o:** Uso de bibliotecas de manipula√ß√£o de PDF (como `PyMuPDF`, `pdfplumber` ou similares) ou HTML para converter os documentos baixados em texto puro (strings).
  * **Express√µes Regulares (Regex):** Aplica√ß√£o massiva da biblioteca `re` para identificar e remover ru√≠dos, como cabe√ßalhos, rodap√©s, n√∫meros de p√°gina, refer√™ncias bibliogr√°ficas e quebras de linha indesejadas.
  * **Pr√©-processamento NLP:** Convers√£o do texto para caixa baixa (lowercasing), remo√ß√£o de caracteres especiais e preparo da tokeniza√ß√£o inicial para otimizar o consumo de mem√≥ria na pr√≥xima fase.

### üß† Etapa 3: Infer√™ncia (SciBERT)

* **Objetivo:** Reconhecimento de Entidades Nomeadas (NER) focadas em jarg√£o cient√≠fico.
* **L√≥gica de Programa√ß√£o e Pontos-Chave:**
  * **Carregamento do Modelo:** Integra√ß√£o com a biblioteca `transformers` (Hugging Face) para instanciar o modelo SciBERT com os pesos ajustados pelo processo de *Fine-Tuning*.
  * **Tokeniza√ß√£o Espec√≠fica:** O texto normalizado √© fatiado usando o tokenizador pr√≥prio do SciBERT, que compreende a morfologia de termos acad√™micos.
  * **Processamento em Lotes (Batching):** Para evitar estouro de mem√≥ria (Out-Of-Memory/OOM), o script divide textos longos em blocos menores (ex: 512 tokens) antes de pass√°-los pelo modelo.
  * **Extra√ß√£o de Tags:** A sa√≠da do modelo (geralmente no formato BIO - Begin, Inside, Outside) √© processada para identificar com precis√£o onde uma entidade come√ßa e termina no texto.

### üèóÔ∏è Etapa 4: P√≥s-processamento e Estrutura√ß√£o
* **Objetivo:** Organiza√ß√£o l√≥gica das entidades extra√≠das.
* **L√≥gica de Programa√ß√£o e Pontos-Chave:**
  * **Reconstru√ß√£o de Palavras:** O SciBERT frequentemente divide palavras complexas em sub-tokens (ex: "micro" e "##biologia"). Este script cont√©m a l√≥gica para concatenar esses fragmentos de volta em palavras leg√≠veis.
  * **Filtragem de Confian√ßa (Thresholding):** Descarta predi√ß√µes do modelo que possuam um grau de certeza muito baixo, reduzindo falsos positivos.
  * **Mapeamento de Rela√ß√µes:** Estrutura as entidades encontradas em dicion√°rios Python (`dict`) ou formato JSON, criando relacionamentos entre autores, metodologias e resultados dentro do mesmo artigo.

### üîó Etapa 5: Enriquecimento
* **Objetivo:** Adicionar metadados e ranqueamento cient√≠fico.
* **L√≥gica de Programa√ß√£o e Pontos-Chave:**
  * **Consumo de APIs Externas:** Realiza chamadas automatizadas √† API do **CrossRef** (enviando o DOI) para recuperar metadados confi√°veis: ano exato de publica√ß√£o, contagem de cita√ß√µes, autores e nome da revista.
  * **C√°lculo do Methodi Ordinatio:** Implementa√ß√£o da l√≥gica matem√°tica para ranquear a relev√¢ncia dos artigos. O script processa as vari√°veis coletadas e aplica a f√≥rmula InOrdinatio:

$$InOrdinatio = \left( \frac{IF}{1000} \right) + \alpha \cdot [10 - (Ano_{atual} - Ano_{publicacao})] + Citacoes$$

  * *(Onde IF √© o Fator de Impacto, Œ± √© o peso atribu√≠do pelo pesquisador, e o ano atual penaliza artigos muito antigos em favor dos mais recentes e citados).*

### üìä Etapa 6: Apresenta√ß√£o
* **Objetivo:** Gera√ß√£o do banco de dados anal√≠tico final.
* **L√≥gica de Programa√ß√£o e Pontos-Chave:**
  * **Agrega√ß√£o de DataFrames:** O script utiliza o `pandas` para cruzar e unir (opera√ß√µes de *merge* e *join*) os dados estruturados do p√≥s-processamento com as m√©tricas calculadas no enriquecimento.
  * **Formata√ß√£o de Sa√≠da:** Renomea√ß√£o de colunas para termos amig√°veis ao usu√°rio final e tratamento de dados nulos (`NaN` ou `None`).
  * **Exporta√ß√£o:** Uso do m√©todo `to_excel` para gerar o arquivo final, garantindo a codifica√ß√£o correta (`utf-8`) para que n√£o haja problemas com acentua√ß√£o na leitura da planilha.
## üî¨ Treinamento e Valida√ß√£o do Modelo (SciBERT)

O desenvolvimento do modelo de Reconhecimento de Entidades Nomeadas (NER) ocorreu em uma esteira paralela ao pipeline principal de processamento. Antes de o orquestrador utilizar o modelo em produ√ß√£o na **Etapa 3 (Infer√™ncia)**, o SciBERT passou por um rigoroso processo de *fine-tuning* e valida√ß√£o metodol√≥gica.

### üìà Avalia√ß√£o de Desempenho
* **Objetivo:** Valida√ß√£o quantitativa e qualitativa da efic√°cia do modelo SciBERT ap√≥s o treinamento em dados rotulados.
* **Justificativa Metodol√≥gica:** A escolha destas t√©cnicas afasta-se da acur√°cia global tradicional (que costuma ser inflada em tarefas de NER devido ao excesso de palavras fora das entidades). O uso do *seqeval* garante rigor ao avaliar a entidade como um todo (esquema BIO). A Matriz de Confus√£o √© essencial para diagnosticar se o modelo est√° confundindo conceitos cient√≠ficos pr√≥ximos (ex: classificando uma Metodologia como Resultado). Por fim, o UMAP atua como prova de aprendizado profundo: ele demonstra visualmente que a IA n√£o est√° apenas memorizando palavras, mas sim compreendendo a dist√¢ncia sem√¢ntica entre os termos no espa√ßo vetorial.
* **L√≥gica de Programa√ß√£o e Pontos-Chave:**
  * **M√©tricas de R√≥tulos de Sequ√™ncia (Seqeval):** Integra√ß√£o com o framework `seqeval` para extrair valores precisos de Precis√£o (Precision), Revoca√ß√£o (Recall) e F1-Score (Micro e Macro) no conjunto de teste. O c√°lculo respeita rigorosamente as fronteiras das entidades extra√≠das. A m√©trica harm√¥nica F1, central para atestar o equil√≠brio do modelo, √© processada pela rela√ß√£o:

$$F_1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$$

  * **An√°lise de Erros (Matriz de Confus√£o):** Constru√ß√£o de matrizes de conting√™ncia utilizando `scikit-learn` e bibliotecas de plotagem (`matplotlib`/`seaborn`). 
  
  A matriz permite a inspe√ß√£o direta da taxa de acertos na diagonal principal e exp√µe detalhadamente os falsos positivos e falsos negativos entre as diferentes categorias sem√¢nticas mapeadas na monografia.
  * **Proje√ß√£o do Espa√ßo Vetorial Latente (UMAP):** Aplica√ß√£o do algoritmo n√£o linear UMAP (*Uniform Manifold Approximation and Projection*) para redu√ß√£o de dimensionalidade. O script extrai os *embeddings* densos gerados pelas √∫ltimas camadas ocultas do SciBERT (vetores de 768 dimens√µes) e os comprime matematicamente para visualiza√ß√£o gr√°fica em 2D.
  
  * **Interpreta√ß√£o Sem√¢ntica Qualitativa:** O agrupamento (*clustering*) resultante do UMAP gera evid√™ncias visuais indispens√°veis para a publica√ß√£o. A separa√ß√£o clara dos *clusters* de dados atesta empiricamente que o modelo aprendeu a distinguir as classes de entidades no espa√ßo sem√¢ntico antes de ser integrado ao orquestrador principal.
