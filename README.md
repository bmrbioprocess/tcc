# TCC
## üß© Arquitetura, L√≥gica e Etapas do Pipeline

O pipeline foi modularizado em blocos independentes para garantir a escalabilidade e facilitar a manuten√ß√£o do c√≥digo. Abaixo est√° o detalhamento t√©cnico de cada etapa acionada pelo orquestrador:

### üì• Etapa 1: Aquisi√ß√£o
* **Objetivo:** Coleta prim√°ria dos dados a partir de identificadores.
* **L√≥gica de Programa√ß√£o e Pontos-Chave:**
  * **Manipula√ß√£o de Dados:** Utiliza a biblioteca `pandas` para ler a planilha Excel inicial e iterar sobre a coluna de DOIs.
  * **Requisi√ß√µes Web:** Emprega bibliotecas como `requests` ou `urllib` para acessar os reposit√≥rios dos artigos.
  * **Tratamento de Exce√ß√µes (Error Handling):** Ponto cr√≠tico do script. Implementa blocos `try-except` para lidar com links quebrados (Erro 404), bloqueios de acesso (Erro 403) ou tempos de limite de conex√£o (Timeouts), garantindo que o orquestrador n√£o pare de funcionar se um download falhar.
  * **Controle de Taxa (Rate Limiting):** Uso de pausas (`time.sleep`) entre as requisi√ß√µes para evitar o bloqueio do IP pelos servidores acad√™micos.

### üßπ Etapa 2: Normaliza√ß√£o
* **Objetivo:** Limpeza e padroniza√ß√£o do texto bruto.
* **L√≥gica de Programa√ß√£o e Pontos-Chave:**
  * **Extra√ß√£o:** Uso de bibliotecas de manipula√ß√£o de PDF (como `PyMuPDF`, `pdfplumber` ou similares) ou HTML para converter os documentos baixados em texto puro (strings).
  * **Express√µes Regulares (Regex):** Aplica√ß√£o massiva da biblioteca `re` para identificar e remover ru√≠dos, como cabe√ßalhos, rodap√©s, n√∫meros de p√°gina, refer√™ncias bibliogr√°ficas e quebras de linha indesejadas.
  * **Pr√©-processamento NLP:** Convers√£o do texto para caixa baixa (lowercasing), remo√ß√£o de caracteres especiais e preparo da tokeniza√ß√£o inicial para otimizar o consumo de mem√≥ria na pr√≥xima fase.

### üß† Etapa 3: Infer√™ncia (SciBERT)

* **Objetivo:** Reconhecimento de Entidades Nomeadas (NER) focadas em jarg√£o cient√≠fico.
* **L√≥gica de Programa√ß√£o e Pontos-Chave:**
  * **Carregamento do Modelo:** Integra√ß√£o com a biblioteca `transformers` (Hugging Face) para instanciar o modelo SciBERT com os pesos ajustados pelo processo de *Fine-Tuning*.
  * **Tokeniza√ß√£o Espec√≠fica:** O texto normalizado √© fatiado usando o tokenizador pr√≥prio do SciBERT, que compreende a morfologia de termos acad√™micos.
  * **Processamento em Lotes (Batching):** Para evitar estouro de mem√≥ria (Out-Of-Memory/OOM), o script divide textos longos em blocos menores (ex: 512 tokens) antes de pass√°-los pelo modelo.
  * **Extra√ß√£o de Tags:** A sa√≠da do modelo (geralmente no formato BIO - Begin, Inside, Outside) √© processada para identificar com precis√£o onde uma entidade come√ßa e termina no texto.

### üèóÔ∏è Etapa 4: P√≥s-processamento e Estrutura√ß√£o
* **Objetivo:** Organiza√ß√£o l√≥gica das entidades extra√≠das.
* **L√≥gica de Programa√ß√£o e Pontos-Chave:**
  * **Reconstru√ß√£o de Palavras:** O SciBERT frequentemente divide palavras complexas em sub-tokens (ex: "micro" e "##biologia"). Este script cont√©m a l√≥gica para concatenar esses fragmentos de volta em palavras leg√≠veis.
  * **Filtragem de Confian√ßa (Thresholding):** Descarta predi√ß√µes do modelo que possuam um grau de certeza muito baixo, reduzindo falsos positivos.
  * **Mapeamento de Rela√ß√µes:** Estrutura as entidades encontradas em dicion√°rios Python (`dict`) ou formato JSON, criando relacionamentos entre autores, metodologias e resultados dentro do mesmo artigo.

### üîó Etapa 5: Enriquecimento
* **Objetivo:** Adicionar metadados e ranqueamento cient√≠fico.
* **L√≥gica de Programa√ß√£o e Pontos-Chave:**
  * **Consumo de APIs Externas:** Realiza chamadas automatizadas √† API do **CrossRef** (enviando o DOI) para recuperar metadados confi√°veis: ano exato de publica√ß√£o, contagem de cita√ß√µes, autores e nome da revista.
  * **C√°lculo do Methodi Ordinatio:** Implementa√ß√£o da l√≥gica matem√°tica para ranquear a relev√¢ncia dos artigos. O script processa as vari√°veis coletadas e aplica a f√≥rmula InOrdinatio:

$$InOrdinatio = \left( \frac{IF}{1000} \right) + \alpha \cdot [10 - (Ano_{atual} - Ano_{publicacao})] + Citacoes$$

  * *(Onde IF √© o Fator de Impacto, Œ± √© o peso atribu√≠do pelo pesquisador, e o ano atual penaliza artigos muito antigos em favor dos mais recentes e citados).*

### üìä Etapa 6: Apresenta√ß√£o
* **Objetivo:** Gera√ß√£o do banco de dados anal√≠tico final.
* **L√≥gica de Programa√ß√£o e Pontos-Chave:**
  * **Agrega√ß√£o de DataFrames:** O script utiliza o `pandas` para cruzar e unir (opera√ß√µes de *merge* e *join*) os dados estruturados do p√≥s-processamento com as m√©tricas calculadas no enriquecimento.
  * **Formata√ß√£o de Sa√≠da:** Renomea√ß√£o de colunas para termos amig√°veis ao usu√°rio final e tratamento de dados nulos (`NaN` ou `None`).
  * **Exporta√ß√£o:** Uso do m√©todo `to_excel` para gerar o arquivo final, garantindo a codifica√ß√£o correta (`utf-8`) para que n√£o haja problemas com acentua√ß√£o na leitura da planilha.
